---
title: "Application of Logistic Regression via packages stats::glm and glmnet for Enhancer Prediction data."
author: 'Shaurya Jauhari (Email: shauryajauhari@gzhmu.edu.cn)'
date: '`r paste(Sys.Date())`'
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This is a R Markdown document on *glmnet* and *stats::glm* packages. These packages provide functionalities to cater to logistic regression problems, amongst others. Let's begin with *glmnet* first.

# glmnet


```{r installing and loading glmnet}
install.packages("glmnet", 
                 repos = "https://cran.us.r-project.org")
library(glmnet)
```

The dataset derivation has been lengthily described [here](https://nbviewer.jupyter.org/github/shauryajauhari/Machine_Learning/blob/master/Machine_Learning_Deep_Learning/enhancer_prediction_dataset_protocol.ipynb)

```{r loading the data}
epdata <- readRDS("../../Machine_Learning_Deep_Learning/predictionData.rds")

## Subsetting just features and class columns.
epdata <- epdata[,4:8]
colnames(epdata)[1:4] <- c("reads_h3k27ac", "reads_h3k4me1", "reads_h3k4me2", "reads_h3k4me3")

```

> You can always seek help in the R documentation with ?.

The class labels 0 and 1 represent "Enhancer" and "Non-Enhancer" categories, respectively.

```{r Cross-validation for finding the optimal lambda.}
set.seed(005)
cv.modelfit <- cv.glmnet(as.matrix(epdata[,1:4]), 
                         epdata$class, 
                         family = "binomial", 
                         type.measure = "class", 
                         alpha = 1, 
                         nlambda = 100)
```


```{r}

plot(cv.modelfit)
cat("There are", length(cv.modelfit$lambda),
    "lambda values in all:",
    cv.modelfit$lambda,
    ", out of which", 
    min(cv.modelfit$lambda), 
    "is the minimum, while",
    cv.modelfit$lambda.1se,
    "denotes the value at which the model is optimized at one standard error.")

```
The plot shows the models (with varying lambda values) that *glmnet* has fit, alongwith the misclassification error associated with each model.The first dotted line highlights the minimum misclassification error, while the second one is the highly regularized model within *1se* (one standard error).

```{r Fitting model | Listing coefficients}

set.seed(2)
modelfit <- glmnet(as.matrix(epdata[,1:4]), 
                   epdata$class, 
                   family = "binomial", 
                   alpha = 1, 
                   lambda = min(cv.modelfit$lambda))

# Listing non-zero coefficients

print(modelfit$beta[,1])
plot(modelfit)
```
Note that the **features must be presented as a data matrix**, while the **response variable is a factor with two levels**. On calling the *glmnet*, we get information under 3 heads: *Df* signifies the number of non-zero coefficients from left to right, i.e. in this case coefficients for reads_h3k27ac, reads_h3k4me3, raeds_h3k4me2, reads_h3k4me1; *%Dev* represents deviation; and *Lambda* represents the penalties imposed by the model. They would typically be limited to 100, but could even halt early if insufficient deviation is observed. Also, by default elastic-net (lasso+ridge) is used for regularization task by the glmnet, which could be set to lasso (alpha=1) or ridge (alpha=0).



```{r exploring coef and predict functions}
coef(modelfit)[,1]
predict(modelfit, type="coef")
```
> "." here symbolizes 0.

***
## Excercises.

1. Try to fit the model with varying lamba, say *cv.modelfit$lambda.1se*.
2. Try the above for alpha = 0, i.e. ridge penalty.
3. Try the above for any value between 0 and 1; that's **elastic-net** regularization.
4. Try the above template for several available datasets at <http://archive.ics.uci.edu/ml/index.php>. 

```{r Excercise 1}
# Model fit with *cv.modelfit$lambda.1se*.
set.seed(5)
modelfit1 <- glmnet(as.matrix(epdata[,1:4]), 
                   epdata$class, 
                   family = "binomial", 
                   alpha = 1, 
                   lambda = cv.modelfit$lambda.1se)

# Listing non-zero coefficients

print(modelfit1$beta[,1])
plot(modelfit1)

```
```{r Excercise 2}
# Model fit with alpha=0.
set.seed(3)
modelfit2 <- glmnet(as.matrix(epdata[,1:4]), 
                   epdata$class, 
                   family = "binomial", 
                   alpha = 0, 
                   lambda = min(cv.modelfit$lambda))

# Listing non-zero coefficients

print(modelfit2$beta[,1])
plot(modelfit2)
```
```{r Excercise 3}
# Model fit with alpha=0.5.
set.seed(05)
modelfit3 <- glmnet(as.matrix(epdata[,1:4]), 
                   epdata$class, 
                   family = "binomial", 
                   alpha = 0.5, 
                   lambda = min(cv.modelfit$lambda))

# Listing non-zero coefficients

print(modelfit3$beta[,1])
plot(modelfit3)
```
 
 
Now, let's move to stats::glm.


#stats::glm()
 
The *stats* package is preloaded in R. We are particularly interested in the generalised linear models , glm() function.
To begin, we shall customarily bifurcate our dataset into training data and testing data. The training data shall be used to build our linear model, while the testing data shall be used for its validation. Arbitrary proportions can be considered for splitting the data, however, usually 80-20 partition is reasonable.


```{r Partition data into proportions: 70% (Train)  and 30% (Test)}
set.seed(7) # for results reproducibility.

epdata$class <- as.numeric(as.factor(epdata$class))-1
part <- sample(2, nrow(epdata), 
               replace = TRUE, 
               prob = c(0.7,0.3))
train <- epdata[part==1,]
test <- epdata[part==2,]
cat("So, now we have", 
    nrow(train), 
    "training rows and",
    nrow(test),
    "testing rows")
```

```{r Structuring the Logistic Regression Model}
epmodel <- glm(formula = class ~ reads_h3k27ac + reads_h3k4me3 + reads_h3k4me2 + reads_h3k4me1, 
               data = train, 
               family = "binomial")
summary(epmodel)
```

Here, we are taking into account all the variables as responses to the predictor variable - *Class*. Although, it can be interpreted straightforwardly, that none of the estimated coefficients of the model are statistically significant (See Pr (>|z|)); but that's just the nature of this data, and in general terms it's better to reject all variables that have insignificant coefficients. Had we chosen to do that here, we would've left with nothing. Never mind. This demonstration is to highlight the protocol of logistic regression. Let's continue with whatever we have here, taking all.
 
Nonetheless, we musn;t ignore an important aspect of *multicollinearity*. Out of many ways to access that, rms::vif() provides an effective way to seek multicollinearity problem. vif stands for Variance Inflation Factor, and by norm if vif() > 10, we must omit the corresponding column (variable) as it does not add much to the model due to redundancy.


```{r Variance Inflation Factor}
install.packages("rms", 
                 repos = "https://cran.us.r-project.org")
library(rms)
vif(epmodel)
```
Neither of the variables have high vif score, so they all qualify for the model.

```{r Model prediction}
y_train <- predict(epmodel, train, type = "response")
head(y_train)
head(train)
```

These are the estimates of the class variable. To calculate the accuracy of the model we need to compare these to the orginal values of the response variable, 0 for "Enhancer" and 1 for "Non-Enhancer". If you see the first observation, 0.6305290 ~ 63 % chance of being a non-enhancer, and in actuality if you look at the original data frame it is a non-enhancer. The probability (63 %) can be calculated by fitting values of coefficients in the model. Try doing that.

y =  0.534487 + (-0.041838 x peaks_h3k27ac) + (0.049430 x reads_h3k4me3) + (0.065430 x reads_h3k4me2) + (0.100927 x reads_h3k4me1)

```{r Misclassification Error}

prediction_probabilities_train <- ifelse(y_train > 0.5, 1, 0) # Probabilities to Labels conversion
confusion_matrix_train <- table(Predicted = prediction_probabilities_train, Actual = train$class)
print(confusion_matrix_train)
misclassfication_error_train <- 1- sum(diag(confusion_matrix_train))/sum(confusion_matrix_train) 
cat("The misclassification error in train data is",
    (round(misclassfication_error_train*100)), "percent")

```

 
Now, we can repeat the same procedure for the test data.
 

```{r For test data}
y_test <- predict(epmodel, test, type = "response")
prediction_probabilities_test <- ifelse(y_test > 0.5, 1, 0)
confusion_matrix_test <- table(Predicted = prediction_probabilities_test, Actual = test$class)
print(confusion_matrix_test)
misclassfication_error_test <- 1- sum(diag(confusion_matrix_test))/sum(confusion_matrix_test) 
cat("The misclassification error in test data is",
    (round(misclassfication_error_test*100)), "percent")
```
 
Finally, there is also a way to ascertain if our model on the whole is statistically significant. We refer this as the Goodness-Of-Fit test.
 
```{r Goodness of Fit test}
overall_p <- with(epmodel, 
                  pchisq(null.deviance-deviance, 
                         df.null-df.residual,
                         lower.tail = FALSE))
cat("The statistical significance for the model is", overall_p, "\n")
cat("The confidence level for this model is", 
    ((1-overall_p)*100), "percent")

```
 
Is this an ideal model ???