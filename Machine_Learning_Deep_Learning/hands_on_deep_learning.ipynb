{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning has conquered our digital space in a much conceivable way. Aritifical Intelligence is the umbrella discipline that describes the varied notions of the systemic behavior akin to humans. Unlike the common held adage, **Deep Learning** isn't a part of Machine Learning per se, rather it has evolved to be an exclusive stream altogether.  \n",
    "If we recall from the previous session, we discussed the relevance of decision trees and random forests and how crucial former is to the latter. The diversity and aggregation of results in the random forests engender them the alacrity to handle multidimensional data. Although, neural nets do not foster the *same* relationship with deep learning, it is of the utmost importance to fathom the concept prior to proceeding to the holistic theme of deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./props/AI_branches.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, deep learning is founded on **neural networks** that have, generally, outperformed other classification algorithms like Support Vector Machines (SVM), logistic regression, etc. *Neural Networks* are a specialized set of algorithms that work on assigning and moderating weights from the original data (input), across through the layers to the classification result (ouput).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./props/deep_learning_NN_better.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "> Loosely, Neural Networks is to Deep Learning as ________ is to Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There could be subjective instances of errors that a system would result, owing to some missing or conflicting dependencies. Few encountered are as under:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./props/install_keras_error.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorFlow is the Google's offering for machine learning and deep learning related tasks; *Anaconda* and *Python* libraries are major requisites for *TensorFlow* deployment in R. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./props/install_keras_error1.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often,\n",
    "- there is a missing declaration in the PATH variable, or \n",
    "- *Anaconda* and *Python* installations have suffered shadowing from the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./props/install_keras_error2.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall selection of data and the analysis protocol has been loosely borrowed from Kim et al. (2016). Typically for a machine learning problem, we shall have a dataset (consolidated, with class and variable/ feature definitions) and that'll be bifurcated (typically in 3:7 or 2:8 proportions) to be used as testing and training sets respectively. Contrarily, in this study both the categories have been sourced differently as you'll see.\n",
    "In this exercise, we shall have positive and negative training examples for the training dataset but only positive examples for the test dataset. And that is perfectively fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing data, we consider data only for enhancers. Even with this data, we shall be able to evaluate the model's veracity of predicting positive examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted, the primary step is to source the BAM files, index them, and finally tranform into BEDGRAPH files. The column naming is done appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3k27ac <- read.csv(\"./data/H1_Cell_Line/H3K27ac/ENCFF663SAM.bw\", sep = '\\t', header = FALSE)\n",
    "colnames(h3k27ac) <- c(\"chrom\",\"start\",\"end\",\"peaks\")\n",
    "h3k4me1 <- read.csv(\"./data/H1_Cell_Line/H3K4me1/ENCFF441KOL.bw\", sep = '\\t', header = FALSE)\n",
    "colnames(h3k4me1) <- c(\"chrom\",\"start\",\"end\",\"peaks\")\n",
    "h3k4me2 <- read.csv(\"./data/H1_Cell_Line/H3K4me2/ENCFF799BDH.bw\", sep = '\\t', header = FALSE)\n",
    "colnames(h3k4me2) <- c(\"chrom\",\"start\",\"end\",\"peaks\")\n",
    "h3k4me3 <- read.csv(\"./data/H1_Cell_Line/H3K4me3/ENCFF340UJK.bw\", sep = '\\t', header = FALSE)\n",
    "colnames(h3k4me3) <- c(\"chrom\",\"start\",\"end\",\"peaks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files (with extension **.bw**) are eventually imported to the online Galaxy interface and executed for the *bedtools Merge* function. The four files showcasing various histone markers for enhancer data are merged together on the basis of overlapping intervals to get a matrix of peak counts that act as features to the 'enhancer' class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant file is eventually imported back to R for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_bw <- read.csv(\"./data/H1_Cell_Line/bedtools_Merge.bw\", sep = '\\t', header = FALSE)\n",
    "merged_bw <- merged_bw[-1,]\n",
    "colnames(merged_bw) <- c(\"chrom\", \"start\", \"end\", \"peaks_h3k4me3\", \"peaks_h3k4me2\", \"peaks_h3k4me1\", \"peaks_h3k27ac\")\n",
    "head(merged_bw)\n",
    "merged_bw$class <- \"enhancer\"\n",
    "head(merged_bw)\n",
    "\n",
    "\n",
    "## Identifying examples of standard chromosomes only and filtering the residuals.\n",
    "\n",
    "chromosomes <- c(\"chr1\",\"chr2\",\"chr3\",\"chr4\",\"chr5\",\"chr6\",\"chr7\",\"chr8\",\"chr9\",\"chr10\",\"chr11\",\"chr12\",\"chr13\",\"chr14\",\n",
    "                 \"chr15\", \"chr16\", \"chr17\", \"chr18\", \"chr19\", \"chr20\",\"chr21\", \"chr22\", \"chrX\", \"chrY\")\n",
    "merged_bw<- as.data.frame(merged_bw[merged_bw$chrom %in% chromosomes, ])\n",
    "\n",
    "\n",
    "## Deriving test data as input to the deep learning model.\n",
    "\n",
    "test <- merged_bw[,c(4:8)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(test)\n",
    "plot(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep learning model has two basic requisites with the input data.\n",
    "1. The data has to be *numeric* in type.\n",
    "2. It has to range from 0 to 1. So if it isn't already, some sort of normalization procedure can help do that. The preferred one is the min-max normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming to numeric(double).\n",
    "\n",
    "test$peaks_h3k4me3 <- as.double(as.character(test$peaks_h3k4me3))\n",
    "test$peaks_h3k4me2 <- as.double(as.character(test$peaks_h3k4me2))\n",
    "test$peaks_h3k4me1 <- as.double(as.character(test$peaks_h3k4me1))\n",
    "test$peaks_h3k27ac <- as.double(as.character(test$peaks_h3k27ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max normalization.\n",
    "\n",
    "test$peaks_h3k4me3 <- (test$peaks_h3k4me3-min(test$peaks_h3k4me3))/(max(test$peaks_h3k4me3)-min(test$peaks_h3k4me3))\n",
    "test$peaks_h3k4me2 <- (test$peaks_h3k4me2-min(test$peaks_h3k4me2))/(max(test$peaks_h3k4me2)-min(test$peaks_h3k4me2))\n",
    "test$peaks_h3k4me1 <- (test$peaks_h3k4me1-min(test$peaks_h3k4me1))/(max(test$peaks_h3k4me1)-min(test$peaks_h3k4me1))\n",
    "test$peaks_h3k27ac <- (test$peaks_h3k27ac-min(test$peaks_h3k27ac))/(max(test$peaks_h3k27ac)-min(test$peaks_h3k27ac))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(test)\n",
    "plot(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kim, S. G., Harwani, M., Grama, A., & Chaterji, S. (2016). EP-DNN : A Deep Neural Network- Based Global Enhancer Prediction Algorithm. Nature Publishing Group, (November), 1â€“13. https://doi.org/10.1038/srep38433"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
